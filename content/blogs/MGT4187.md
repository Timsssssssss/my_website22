---
categories:
- ""
- ""
date: "2017-10-31T22:26:13-05:00"
description: 
draft: false
image: weibo.jpg
keywords: ""
slug: weibo
title: How Did ANTA Win During the Olympics?
---
This is my Python code for Sentiment Analysis for ANTA during the Olympics. The analysis was used to study and evaluate ANTA's marketing strategy during the Winter Olympics in 2022. For an analytic report, please see [Link](https://github.com/Timsssssssss/my_website22/blob/main/content/blogs/Analytical_Report.pdf)



```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from snownlp import SnowNLP
import datetime
import jieba.analyse
import re
from wordcloud import WordCloud
sns.set_style("darkgrid")
```


```python
jieba.load_userdict('userdict.txt')
```


```python
df = pd.read_csv("weibo.csv")
```


```python
df['publish_time'] = pd.to_datetime(df['publish_time'],format='%Y-%m-%d')
```

# Clean the context


```python
#Sorce:  https://blog.csdn.net/qq_43814415/article/details/119517978
def clean(line):
    rep=['ã€ã€‘','ã€','ã€‘','ğŸ‘','ğŸ¤',
        'ğŸ®','ğŸ™','ğŸ‡¨ğŸ‡³','ğŸ‘','â¤ï¸','â€¦â€¦â€¦','ğŸ°','...ã€ã€','ï¼Œï¼Œ','..','ğŸ’ª','ğŸ¤“',
         'âš•ï¸','ğŸ‘©','ğŸ™ƒ','ğŸ˜‡','ğŸº','ğŸ‚','ğŸ™ŒğŸ»','ğŸ˜‚','ğŸ“–','ğŸ˜­','âœ§Ù©(ËŠÏ‰Ë‹*)Ùˆâœ§','ğŸ¦','ï¼Ÿï¼Ÿï¼Ÿï¼Ÿ','//','ğŸ˜Š','ğŸ’°','ğŸ˜œ','ğŸ˜¯',
         '(áƒ¦Ë˜âŒ£Ë˜áƒ¦)','âœ§ï¼¼Ù©(ëˆˆà±ªëˆˆ)Ùˆ/ï¼âœ§','ğŸŒ','ğŸ€','ğŸ´',
         'ğŸŒ»','ğŸŒ±','ğŸŒ±','ğŸŒ»','ğŸ™ˆ','(à¸‡â€¢Ì€_â€¢Ì)à¸‡ï¼','ğŸ‰‘ï¸','ğŸ’©',
         'ğŸ','âŠ™âˆ€âŠ™ï¼','ğŸ™Š','ã€ï¼Ÿ','+1','ğŸ˜„','ğŸ™','ğŸ‘‡ğŸ»','ğŸ“š','ğŸ™‡',
         'ğŸ™‹','ï¼ï¼ï¼ï¼','ğŸ‰','ï¼¼(^â–½^)ï¼','ğŸ‘Œ','ğŸ†’','ğŸ»',
         'ğŸ™‰','ğŸµ','ğŸˆ','ğŸŠ','0371-12345','â˜•ï¸','ğŸŒ','ğŸ˜³','ğŸ‘»','ğŸ¶','ğŸ‘„','\U0001f92e\U0001f92e','ğŸ˜”','ï¼‹1','ğŸ›€','ğŸ¸','ğŸ·','â•1',
         'ğŸŒš','ï¼šï¼š','ğŸ’‰','âˆš','x','ï¼ï¼ï¼','ğŸ™…','â™‚ï¸','ğŸ’Š','ğŸ‘‹','o(^o^)o','mei\u2006sha\u2006shi','ğŸ’‰','ğŸ˜ª','ğŸ˜±',
         'ğŸ¤—','å…³æ³¨','â€¦â€¦','(((â•¹Ğ´â•¹;)))','âš ï¸','Ô¾â€¸Ô¾','â›½ï¸','ğŸ˜“','ğŸµ',
         'ğŸ™„ï¸','ğŸŒ•','â€¦','ğŸ˜‹','[]','[',']','â†’_â†’','ğŸ’','ğŸ˜¨','&quot;','ğŸ˜','à¸…Û¶â€¢ï»Œâ€¢â™¡','ğŸ˜°','ğŸ™ï¸',
         'ğŸ¤§','ğŸ˜«','(à¸‡â€¢Ì€_â€¢Ì)à¸‡','ğŸ˜','âœŠ','ğŸš¬','ğŸ˜¤','ğŸ‘»','ğŸ˜£','ï¼š','ğŸ˜·','(*^â–½^)/â˜…*â˜†','ğŸ','ğŸ”','ğŸ˜˜','ğŸ‹','(âœªâ–½âœª)','(âÂ´Ï‰`â)','1âƒ£3âƒ£','(^_^)ï¼','â˜€ï¸',
	     'ğŸ','ğŸ˜…','ğŸŒ¹','ğŸ ','â†’_â†’','ğŸ™‚','âœ¨','â„ï¸','â€¢','ğŸŒ¤','ğŸ’“','ğŸ”¨','ğŸ‘','ğŸ˜','âŠ™âˆ€âŠ™ï¼','ğŸ‘','âœŒ(Ì¿â–€Ì¿\u2009Ì¿Ä¹Ì¯Ì¿Ì¿â–€Ì¿Ì¿)âœŒ',
         'ğŸ˜Š','ğŸ‘†','ğŸ’¤','ğŸ˜˜','ğŸ˜Š','ğŸ˜´','ğŸ˜‰','ğŸŒŸ','â™¡â™ª..ğ™œğ™¤ğ™¤ğ™™ğ™£ğ™ğ™œğ™ğ™©â€¢Íˆá´—â€¢Íˆâœ©â€§â‚ŠËš','ğŸ‘ª','ğŸ’°','ğŸ˜','ğŸ€','ğŸ›','ğŸ–•ğŸ¼','ğŸ˜‚','(âœªâ–½âœª)','ğŸ‹','ğŸ…','ğŸ‘€','â™‚ï¸','ğŸ™‹ğŸ»','âœŒï¸','ğŸ¥³','ï¿£ï¿£)Ïƒ',
         'ğŸ˜’','ğŸ˜‰','ğŸ¦€','ğŸ’–','âœŠ','ğŸ’ª','ğŸ™„','ğŸ£','ğŸŒ¾','âœ”ï¸','ğŸ˜¡','ğŸ˜Œ','ğŸ”¥','â¤','ğŸ¼','ğŸ¤­','ğŸŒ¿','ä¸¨','âœ…','ğŸ¥','ï¾‰','â˜€','5âƒ£âº1âƒ£0âƒ£','ğŸš£','ğŸ£','ğŸ¤¯','ğŸŒº',
         'ğŸŒ¸','\u200b','\ue627OOTD','æ”¶èµ·d'
         ]
    pattern_0=re.compile('#.*?#')#åœ¨ç”¨æˆ·åå¤„åŒ¹é…è¯é¢˜åç§°
    pattern_1=re.compile('ã€.*?ã€‘')#åœ¨ç”¨æˆ·åå¤„åŒ¹é…è¯é¢˜åç§°
    pattern_2=re.compile('è‚ºç‚@([\u4e00-\u9fa5\w\-]+)')#åŒ¹é…@
    pattern_3=re.compile('@([\u4e00-\u9fa5\w\-]+)')#åŒ¹é…@
    #è‚ºç‚@ç¯çƒæ—¶æŠ¥
    pattern_4=re.compile(u'[\U00010000-\U0010ffff\uD800-\uDBFF\uDC00-\uDFFF]')#åŒ¹é…è¡¨æƒ…
    pattern_7=re.compile('L.*?çš„å¾®åšè§†é¢‘')
    pattern_8=re.compile('ï¼ˆ.*?ï¼‰')
    #pattern_9=re.compile(u"\|[\u4e00-\u9fa5]*\|")#åŒ¹é…ä¸­æ–‡

    line=line.replace('Oç½‘é¡µé“¾æ¥','')
    line=line.replace('-----','')
    line=line.replace('â‘ ','')
    line=line.replace('â‘¡','')
    line=line.replace('â‘¢','')
    line=line.replace('â‘£','')
    line=line.replace('>>','')
    line=re.sub(pattern_0, '', line,0) #å»é™¤è¯é¢˜
    line=re.sub(pattern_1, '', line,0) #å»é™¤ã€ã€‘
    line=re.sub(pattern_2, '', line,0) #å»é™¤@
    line=re.sub(pattern_3, '', line,0) #å»é™¤@
    line=re.sub(pattern_4, '', line,0) #å»é™¤è¡¨æƒ…
    line=re.sub(pattern_7, '', line,0) 
    line=re.sub(pattern_8, '', line,0) 
    line=re.sub(r'\[\S+\]', '', line,0) #å»é™¤è¡¨æƒ…ç¬¦å·
    
    for i in rep:
        line=line.replace(i,'')
    return line

```


```python
df['content'] = df.apply(lambda x:clean(x['content']),axis = 1)
```

# Data Inspection


```python
df['voice'] = df['forward_num']+df['comment_num']+df['like_num']
V = pd.DataFrame()
```


```python
V['voice'] = df.groupby('publish_time').sum()['voice']
V['num'] = df.groupby('publish_time').count()['voice']
```


```python
V['voice'].plot(figsize=(14,7))
```




    <AxesSubplot:xlabel='publish_time'>




    
![Image](output_10_1.png)
    



```python
V['num'].plot(figsize=(14,7))
```




    <AxesSubplot:xlabel='publish_time'>




    
![Image](output_11_1.png)
    



```python
V.to_csv('å®‰è¸å¾®åšå£°é‡.csv',index= True)
```

# Sentiment Analysis


```python
def cal_score(df):
    n = len(df['content'])
    sentiment = np.empty(n)
    for i in range(n):
        text = SnowNLP(df['content'][i])
        sent = text.sentences
        score = list()
        for sen in sent:
            s = SnowNLP(sen)
            score.append(s.sentiments)

        sentiment[i] = np.nanmean(score)
    return sentiment
```


```python
df['sentiment'] = cal_score(df)
```

    C:\Users\ty\AppData\Local\Temp/ipykernel_8532/3858428932.py:12: RuntimeWarning: Mean of empty slice
      sentiment[i] = np.nanmean(score)
    


```python
df['sentiment'].describe()
```




    count    13280.000000
    mean         0.654502
    std          0.215907
    min          0.000000
    25%          0.518764
    50%          0.667780
    75%          0.816982
    max          1.000000
    Name: sentiment, dtype: float64




```python
plt.figure(figsize=(12,6))
sns.histplot(df['sentiment'],kde = True)
```




    <AxesSubplot:xlabel='sentiment', ylabel='Count'>




    
![Image](output_17_1.png)
    



```python
print('positive:',len(df[df['sentiment'] >= 0.8]))
print('negative:',len(df[df['sentiment'] <= 0.2]))
```

    positive: 4301
    negative: 485
    


```python
A = pd.DataFrame()
A['Total'] = df.groupby('publish_time').count()['sentiment']
A['Positive'] = df[df['sentiment']>=0.8].groupby('publish_time').count()['sentiment']
A['Negative'] = df[df['sentiment']<=0.2].groupby('publish_time').count()['sentiment']
```


```python
A[['Positive','Negative']].plot(figsize=(14,7))
```




    <AxesSubplot:xlabel='publish_time'>




    
![Image](output_20_1.png)
    



```python
A.to_csv('å®‰è¸æ¯æ—¥æ­£è´Ÿæƒ…æ„Ÿ.csv',index = True)
```


```python
df[['sentiment']].to_csv('å®‰è¸å¾®åšæƒ…æ„Ÿåˆ†æ.csv',index = False)
```


```python
wo = df[(df['publish_time'] >= '2022-02-04')&(df['publish_time'] <= '2022-02-20')].reset_index()
print('positive:',len(wo[wo['sentiment'] >= 0.8]))
print('negative:',len(wo[wo['sentiment'] <= 0.2]))
```

    positive: 2597
    negative: 170
    

# Topic Analysis


```python
def Get_Key_Words(df,Num = 10,only_adv = False):
    segments = []
    for index, row in df.iterrows(): 
        content = row['content']
        content = content.replace('ä¸­å›½é˜Ÿ','')
        content = content.replace('åŠ æ²¹','')
        content = content.replace('å†¬å¥¥','')
        content = content.replace('å¥¥è¿','')
        content = content.replace('å¥å„¿','')
        if only_adv:
            words = jieba.analyse.extract_tags(content, topK=20, allowPOS=('a','ad','vn','nt'))
        else:
            words = jieba.analyse.extract_tags(content, topK=20, allowPOS=('nr', 'n', 'vn', 'v','nz','nt','a'))
        for word in words:
            segments.append({'word':word, 'count':1})
    dfSeg = pd.DataFrame(segments)
    dfWord = dfSeg.groupby('word').sum()[['count']].sort_values('count',ascending=False).reset_index().head(Num)
    return dfWord
```


```python
# Source: https://www.bbsmax.com/A/q4zVPxgW5K/

def create_word_cloud(df,num = 15,wid = 4000,hei = 2000):
    frequencies = {}
    for index,row in df.iterrows():
        frequencies[row['word']] = float(row['count'])
    wc = WordCloud(
        font_path="./SimHei.ttf",
        max_words=num,
        width=wid, 
        height=hei,
        margin=2,
        background_color='white',
        prefer_horizontal=1.5
    )
    word_cloud = wc.generate_from_frequencies(frequencies)
    plt.imshow(word_cloud)
    plt.axis("off")
    plt.show()


```


```python
wo_topic = Get_Key_Words(wo,30)
create_word_cloud(wo_topic)
```


    
![Image](output_27_0.png)
    



```python
df2 = df.copy()
```


```python
df2 = df2[(~df2['content'].str.contains('å›´è§‚å†¬å¥¥'))&(~df2['content'].str.contains('å†¬å¥¥èƒ½é‡'))&(~df2['content'].str.contains('çº¢åŒ…'))&
         (~df2['content'].str.contains('å“ç‰Œæ—¶åˆ»'))]
```


```python
t_1_18 = df2[((df2['publish_time'] == '2022-01-18')|(df2['publish_time'] == '2022-01-21'))&(df2['sentiment'] <= 0.2)].reset_index()
```


```python
create_word_cloud(Get_Key_Words(t_1_18,30))
```


    
![Image](output_31_0.png)
    



```python
df2.groupby('publish_time').count().sort_values('voice',ascending=False)[['voice']].head(6)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>voice</th>
    </tr>
    <tr>
      <th>publish_time</th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2022-02-05</th>
      <td>1407</td>
    </tr>
    <tr>
      <th>2022-02-14</th>
      <td>705</td>
    </tr>
    <tr>
      <th>2022-02-06</th>
      <td>624</td>
    </tr>
    <tr>
      <th>2022-02-20</th>
      <td>623</td>
    </tr>
    <tr>
      <th>2022-02-15</th>
      <td>608</td>
    </tr>
    <tr>
      <th>2022-02-18</th>
      <td>599</td>
    </tr>
  </tbody>
</table>
</div>




```python
t_2_20 = df2[(df2['publish_time'] == '2022-02-20')].reset_index()
t_2_15 = df2[(df2['publish_time'] == '2022-02-15')].reset_index()
t_2_5 = df2[df2['publish_time'] == '2022-02-05'].reset_index()
neg =  wo[wo['sentiment'] <= 0.2].reset_index()
post_mon = df2[(df2['publish_time'] >= '2022-03-01')].reset_index()
```


```python
create_word_cloud(Get_Key_Words(t_2_20,30))
```


    
![Image](output_34_0.png)
    



```python
create_word_cloud(Get_Key_Words(t_2_15,30))
```


    
![Image](output_35_0.png)
    



```python
create_word_cloud(Get_Key_Words(t_2_5,30))
```


    
![Image](output_36_0.png)
    



```python
create_word_cloud(Get_Key_Words(neg,30))
```


    
![Image](output_37_0.png)
    



```python
create_word_cloud(Get_Key_Words(post_mon,30))
```


    
![Image](output_38_0.png)
    


# Hongxing Erke


```python
erke = pd.read_csv("erke.csv")
erke['publish_time'] = pd.to_datetime(erke['publish_time'],format='%Y-%m-%d')
```


```python
erke['content'] = erke.apply(lambda x:clean(x['content']),axis = 1)
```


```python
erke.head(5)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>publish_time</th>
      <th>user_name</th>
      <th>content</th>
      <th>weibo_link</th>
      <th>forward_num</th>
      <th>comment_num</th>
      <th>like_num</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2021-08-31</td>
      <td>æ©˜å®¶æ…§å­</td>
      <td>è‰ç‡äº†ï¼Œä¹°äº†é‚£ä¹ˆå¤šåŒé¸¿æ˜Ÿå°”å…‹æ‹¢é¾™é‹å­ä»£è¨€è¿™ä¸å°±æ¥äº†å˜›çœŸè¦åŒ–èº«èœˆèš£ç²¾äº†</td>
      <td>https://weibo.com/5170218722/Kw3Bo7Uva?refer_f...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2021-08-31</td>
      <td>æŒšç ”å®å¾®</td>
      <td>æŒšç ”å®å¾®å®‰è¸ï¼ŒçœŸæ­£çš„è¿åŠ¨å“ç‰Œä¹‹å…‰  æèµ·å›½å†…çš„è¿åŠ¨å“ç‰Œï¼Œç›¸ä¿¡å¾ˆå¤šäººéƒ½èƒ½å«å‡ºæå®ã€å®‰è¸ã€ç‰¹æ­¥ç­‰...</td>
      <td>https://weibo.com/7475419357/Kw3uGk9RE?refer_f...</td>
      <td>0</td>
      <td>0</td>
      <td>69</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2021-08-31</td>
      <td>é˜¿äº®é™ªä½ çœ‹ä¸–ç•Œ</td>
      <td>è‰¯å¿ƒä¼ä¸šï¼Œå€¼å¾—èµæ‰¬ï¼ä¸€æ–¹æœ‰éš¾ï¼Œå…«æ–¹æ”¯æ´ï¼Œè¿™å°±æ˜¯ä¸­å›½åŠ›é‡ï¼å®å®åœ¨åœ¨èµäº† ä¸€æŠŠï¼</td>
      <td>https://weibo.com/2500620180/Kw3uG1sQy?refer_f...</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2021-08-31</td>
      <td>æˆ‘çš„-hobby</td>
      <td>é¸¿æ˜Ÿå°”å…‹ä¹°çš„é‹å­ï¼Œç°åœ¨ç©¿æ„Ÿè§‰åšäº†ç‚¹</td>
      <td>https://weibo.com/7555559185/Kw323zr5b?refer_f...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2021-08-31</td>
      <td>åƒ-ç›¼-å¦</td>
      <td>çƒ­åº¦è¿‡å»äº†è¿™ä¹ˆä¹…ï¼Œæˆ‘æ‰æ¥æ”¯æŒ â€œé¸¿æ˜Ÿå°”å…‹â€</td>
      <td>https://weibo.com/2533083211/Kw2P1EFrP?refer_f...</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>




```python
erke['voice'] = erke['forward_num']+erke['comment_num']+erke['like_num']
E = pd.DataFrame()
```


```python
E['voice'] = erke.groupby('publish_time').sum()['voice']
E['num'] = erke.groupby('publish_time').count()['voice']
```


```python
E['voice'].plot(figsize=(14,7))
```




    <AxesSubplot:xlabel='publish_time'>




    
![Image](output_45_1.png)
    



```python
E.to_csv('é¸¿æ˜Ÿå°”å…‹å¾®åšå£°é‡.csv',index = True)
```

## Sentiment Analysis


```python
erke['sentiment'] = cal_score(erke)
```

    C:\Users\ty\AppData\Local\Temp/ipykernel_62248/3858428932.py:12: RuntimeWarning: Mean of empty slice
      sentiment[i] = np.nanmean(score)
    


```python
t = erke[(erke['publish_time']>= '2021-07-21')&(erke['publish_time']<= '2021-07-25')]
```


```python
tp = erke[erke['publish_time']> '2021-08-01']
pt =  erke[erke['publish_time']< '2021-07-20']
```


```python
plt.figure(figsize=(12,6))
sns.histplot(t['sentiment'],kde = True)
```




    <AxesSubplot:xlabel='sentiment', ylabel='Count'>




    
![Image](output_51_1.png)
    



```python
t[['sentiment']].to_csv('é¸¿æ˜Ÿå°”å…‹æƒ…æ„Ÿåˆ†æ.csv',index = False)
```


```python
print('positive:',len(t[t['sentiment'] >= 0.8]))
print('negative:',len(t[t['sentiment'] <= 0.2]))
```

    positive: 152
    negative: 191
    

## Topic Analysis


```python
t_pos = t[t['sentiment'] >= 0.8]
t_neg = t[t['sentiment'] <= 0.2]
```


```python
create_word_cloud(Get_Key_Words(t_pos,30,True),15,1000,500)
```


    
![Image](output_56_0.png)
    



```python
create_word_cloud(Get_Key_Words(t_neg,30,True),15,1000,500)
```


    
![Image](output_57_0.png)
    



```python
create_word_cloud(Get_Key_Words(tp,30))
```


    
![Image](output_58_0.png)
    



```python

```
