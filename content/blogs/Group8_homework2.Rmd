---
title: "Study Group 8 - Homework 2"
author: "Misha Aggarwal, Madalina Dumitrescu, Yung-Chieh Hsu, Wendy Li, Christoph Plachutta, Tianyi Zhang"
date: "`r Sys.Date()`"
output:
  html_document:
    theme: flatly
    highlight: zenburn
    number_sections: yes
    toc: yes
    toc_float: yes
    code_folding: show
editor_options: 
  markdown: 
    wrap: 72
---

```{r, setup, include=FALSE}
knitr::opts_chunk$set(
  message = FALSE, 
  warning = FALSE, 
  tidy=FALSE,     # display code as typed
  size="small")   # slightly smaller font for code
options(digits = 3)

# default figure size
knitr::opts_chunk$set(
  fig.width=6.75, 
  fig.height=6.75,
  fig.align = "center"
)
```

```{r load-libraries, include=FALSE}
library(tidyverse)  # Load ggplot2, dplyr, and all the other tidyverse packages
library(mosaic)
library(ggthemes)
library(lubridate)
library(here)
library(skimr)
library(janitor)
library(httr)
library(readxl)
library(vroom)
library(wbstats)
library(countrycode)
library(patchwork)
library(gganimate)
library(infer)
```

# Climate change and temperature anomalies

If we wanted to study climate change, we can find data on the *Combined
Land-Surface Air and Sea-Surface Water Temperature Anomalies* in the
Northern Hemisphere at [NASA's Goddard Institute for Space
Studies](https://data.giss.nasa.gov/gistemp). The [tabular data of
temperature anomalies can be found
here](https://data.giss.nasa.gov/gistemp/tabledata_v4/NH.Ts+dSST.txt)

To define temperature anomalies you need to have a reference, or base,
period which NASA clearly states that it is the period between
1951-1980.

Run the code below to load the file:

```{r weather_data, cache=TRUE}

weather <- 
  read_csv("https://data.giss.nasa.gov/gistemp/tabledata_v4/NH.Ts+dSST.csv", 
           skip = 1, 
           na = "***")

```

Notice that, when using this function, we added two options: `skip` and
`na`.

1.  The `skip=1` option is there as the real data table only starts in
    Row 2, so we need to skip one row.
2.  `na = "***"` option informs R how missing observations in the
    spreadsheet are coded. When looking at the spreadsheet, you can see
    that missing data is coded as "\*\*\*". It is best to specify this
    here, as otherwise some of the data is not recognized as numeric
    data.

Once the data is loaded, notice that there is a object titled `weather`
in the `Environment` panel. If you cannot see the panel (usually on the
top-right), go to `Tools` \> `Global Options` \> `Pane Layout` and tick
the checkbox next to `Environment`. Click on the `weather` object, and
the dataframe will pop up on a seperate tab. Inspect the dataframe.

For each month and year, the dataframe shows the deviation of
temperature from the normal (expected). Further the dataframe is in wide
format.

You have two objectives in this section:

1.  Select the year and the twelve month variables from the `weather`
    dataset. We do not need the others (J-D, D-N, DJF, etc.) for this
    assignment. Hint: use `select()` function.

2.  Convert the dataframe from wide to 'long' format. Hint: use
    `gather()` or `pivot_longer()` function. Name the new dataframe as
    `tidyweather`, name the variable containing the name of the month as
    `month`, and the temperature deviation values as `delta`.

```{r tidyweather}

# Select only year and month data from the weather data set
weather_clean <- weather %>% 
  select(1:13)
  
# Convert dataframe from wide to long format
tidyweather <- weather_clean %>% 
  pivot_longer(cols = 2:13,
               names_to = "month",
               values_to = "delta") %>% 
  rename("year" = "Year")
  
```

Inspect your dataframe. It should have three variables now, one each for

1.  year,
2.  month, and
3.  delta, or temperature deviation.

## Plotting Information

Let us plot the data using a time-series scatter plot, and add a
trendline. To do that, we first need to create a new variable called
`date` in order to ensure that the `delta` values are plot
chronologically.

> In the following chunk of code, I used the `eval=FALSE` argument,
> which does not run a chunk of code; I did so that you can knit the
> document before tidying the data and creating a new dataframe
> `tidyweather`. When you actually want to run this code and knit your
> document, you must delete `eval=FALSE`, **not just here but in all
> chunks were `eval=FALSE` appears.**

```{r scatter_plot}

# Add date column to dataframe
tidyweather <- tidyweather %>%
  mutate(date = ymd(paste(as.character(year), month, "1")),
         month = month(date, label = TRUE),
         year = year(date))
tidyweather

# Plot scatterplot of delta over time with best fit line
ggplot(tidyweather, aes(x=date, y = delta))+
  geom_point()+
  geom_smooth(color="red") +
  theme_bw() +
  labs (
    title = "Weather Anomalies",
    x = "Date",
    y = "Delta")

```

Is the effect of increasing temperature more pronounced in some months?
Use `facet_wrap()` to produce a seperate scatter plot for each month,
again with a smoothing line. Your chart should human-readable labels;
that is, each month should be labeled "Jan", "Feb", "Mar" (full or
abbreviated month names are fine), not `1`, `2`, `3`.

```{r facet_wrap, echo=FALSE}

# Create faceted scatterplot for delta over years by month
tidyweather %>% 
  ggplot(aes(x = year, y = delta)) +
    geom_point() +
    geom_smooth() +
    facet_wrap(~ month) +
    theme_bw() +
    labs(title = "Weather deviations have seen a significant increase in all months since 1880", 
           subtitle = "Weather deviations from 1880 to 2022 in the Northern Hemisphere",
           x = "Year",
           y = "Delta")

```

It is sometimes useful to group data into different time periods to
study historical data. For example, we often refer to decades such as
1970s, 1980s, 1990s etc. to refer to a period of time. NASA calculates
a temperature anomaly as difference from the base period of 1951-1980.
The code below creates a new data frame called `comparison` that groups
data in five time periods: 1881-1920, 1921-1950, 1951-1980, 1981-2010
and 2011-present.

We remove data before 1881 and before using `filter`. Then, we use the
`mutate` function to create a new variable `interval` which contains
information on which period each observation belongs to. We can assign
the different periods using `case_when()`.

```{r intervals}

comparison <- tidyweather %>% 
  filter(year >= 1881) %>% # remove years prior to 1881
  # Create new variable 'interval', and assign values based on criteria below:
  mutate(interval = case_when(
    year %in% c(1881:1920) ~ "1881-1920",
    year %in% c(1921:1950) ~ "1921-1950",
    year %in% c(1951:1980) ~ "1951-1980",
    year %in% c(1981:2010) ~ "1981-2010",
    TRUE ~ "2011-present"
  ))

```

Inspect the `comparison` dataframe by clicking on it in the
`Environment` pane.

Now that we have the `interval` variable, we can create a density plot
to study the distribution of monthly deviations (`delta`), grouped by
the different time periods we are interested in. Set `fill` to
`interval` to group and colour the data by different time periods.

```{r density_plot}

# Create density plot separated by all time periods
comparison %>% 
  ggplot(aes(x = delta, fill = interval)) +
    geom_density(alpha = 0.3) + # Change alpha to make lines more visible
    theme_bw() +
    labs(title = "The average weather deviation is increasing from period to period", 
           subtitle = "Distribution of weather deviations by period",
           x = "Delta",
           y = "Distribution")

```

So far, we have been working with monthly anomalies. However, we might
be interested in average annual anomalies. We can do this by using
`group_by()` and `summarise()`, followed by a scatter plot to display
the result.

```{r averaging}

# Creating yearly averages
average_annual_anomaly <- tidyweather %>% 
  group_by(year) %>%   # Grouping data by Year
  # Creating summaries for mean delta 
  # Use `na.rm=TRUE` to eliminate NA (not available) values 
  summarise(anl_mean_delta = mean(delta), na.rm = TRUE) 

# Create a scatterplot of the annual mean delta over years
average_annual_anomaly %>% 
  ggplot(aes(x = year, y = anl_mean_delta)) +
  geom_point() +
  geom_smooth() +
  theme_bw() +
  labs(title = "Climate in the Northern Hemisphere has been increasing exponentially since 1960", 
           subtitle = "Development of annual weather deviations from 1881 to 2022",
           x = "Years",
           y = "Average Annual Delta")

```

## Confidence Interval for `delta`

[NASA points out on their
website](https://earthobservatory.nasa.gov/world-of-change/decadaltemp.php)
that

> A one-degree global change is significant because it takes a vast
> amount of heat to warm all the oceans, atmosphere, and land by that
> much. In the past, a one- to two-degree drop was all it took to plunge
> the Earth into the Little Ice Age.

Your task is to construct a confidence interval for the average annual
delta since 2011, both using a formula and using a bootstrap simulation
with the `infer` package. Recall that the dataframe `comparison` has
already grouped temperature anomalies according to time intervals; we
are only interested in what is happening between 2011-present.

```{r, calculate_CI_using_formula}

# Create confidence interval per year since 2011 with the formula
formula_ci <- comparison %>%
  # Choose the interval 2011-present
  filter(interval == "2011-present") %>% 
  # Remove NA values in 2022 to compute summary statistics and CI
  drop_na() %>% 
  # Calculate mean, SD, count, SE, lower/upper 95% CI
  summarise(mean_delta = mean(delta),
            median_delta = median(delta),
            sd_delta = sd(delta),
            count = n(),
            t_critical = qt(0.975, count-1),
            se_delta = sd_delta/sqrt(count),
            margin_of_error = t_critical * se_delta,
            delta_low = mean_delta - margin_of_error,
            delta_high = mean_delta + margin_of_error,)

# Print out formula_ci
formula_ci

```

```{r calculate_CI_using_infer}

# Set seed for reproducibility
set.seed(1234)

delta_2011_bootstrap <- comparison %>% 
  # Choose the interval 2011-present
  filter(interval == "2011-present") %>%
  # Specify the variable of interest
  specify(response = delta) %>%
  # Generate bootstrap samples
  generate(reps = 1000, type = "bootstrap") %>%
  # Find the mean of each sample
  calculate(stat = "mean")

# Create the confidence interval of bootstrapped samples
bootstrap_ci <- delta_2011_bootstrap %>% 
  get_confidence_interval(level = 0.95, type = "percentile")

# Print out bootstrap_ci
bootstrap_ci
```


Looking at the 95% confidence interval constructed with either formulas or the infer package, we observe identical values for the interval min and max values. To create the confidence interval with the formula approach, we started by filtering for the time period of interest and removed the months for which we do not yet have data in 2022. We then proceeded to calculate the summary statistics mean, median, standard deviation and count to set the basis for our confidence interval. As mean and median are very similar, we can assume a normal distribution of values. Given the sample size of 139, we have a t_critical value rather close to 1.96 (the t_critical for the bootleg approach will be even closer, given the sample size of 1000). We used the `r qt()`function with a confidence level of 95%, deducting 1 from the count to represent the degree of freedom. As we want to make assertions about the population with the confidence interval, we calculate the standard error and margin of error. By deducting and adding the margin of error to the mean in turn, we are able to develop both the min and max margin of the two-sided confidence interval. The resulting min and max values enable us to claim with a 95% confidence that the temperature in the Northern Hemisphere was between 1.02 and 1.11 degrees Celsius higher since 2011 than the baseline temperature (1951 to 1980). This implies that even at the minimum value of our confidence interval, we would surpass the critical threshold of 1 degree change as defined by NASA. Perhaps climate change is not a [hoax](https://www.forbes.com/sites/markjoyella/2022/03/21/on-fox-donald-trump-calls-climate-change-a-hoax-in-the-1920s-they-were-talking-about-global-freezing/) after all.

# Biden's Approval Margins

As we saw in class, fivethirtyeight.com has detailed data on [all polls
that track the president's approval](https://projects.fivethirtyeight.com/biden-approval-ratings)

```{r, cache=TRUE}

# Import approval polls data directly off fivethirtyeight website
approval_polllist <- read_csv('https://projects.fivethirtyeight.com/biden-approval-data/approval_polllist.csv') 
glimpse(approval_polllist)

```

# Create a plot

What I would like you to do is to calculate the average net approval
rate (approve- disapprove) for each week since he got into office. I
want you plot the net approval for each week in 2022, along with its 95%
confidence interval. There are various dates given for each poll, please
use `enddate`, i.e., the date the poll ended. Your plot should look
something like this:

```{r approval_example, echo=FALSE, out.width="100%"}
knitr::include_graphics(here::here("images", "biden_approval_margin.png"), error = FALSE)
```

```{r biden_approval_plot, fig.height = 4}

approval_polllist %>% 
  #convert end-date into "date" format:
  mutate(date_poll = mdy(enddate)) %>% 
  #extract year from date
  mutate(Year_poll = year(date_poll)) %>%
  #get week number from date 
  mutate(week_no = isoweek(date_poll)) %>%
  #filter out data for year 2022 and eliminate week 52
  filter(Year_poll== 2022, week_no != 52) %>%
  #week-wise average approval rate line plot for different sub-groups 
  group_by(subgroup,week_no) %>% 
  #get mean approval rate (difference of approve & disapprove), standard deviation of approval rate and total number   of votes in each group
  summarize(approval_rate = mean(approve - disapprove), approval_sd = sd(approve-disapprove), n = n()) %>% 
  ungroup(subgroup,week_no) %>% 
  #confidence interval
  mutate(low = approval_rate - 1.96*approval_sd/sqrt(n)) %>%
  mutate(high = approval_rate + 1.96*approval_sd/sqrt(n)) %>%
  #Plot the graph of week number vs approval rate 
  ggplot(aes(x=week_no,y=approval_rate, color = subgroup))+
  geom_line()+
    facet_wrap(~subgroup, nrow = 3, strip.position = "right")+
    xlim(0,35)+
    geom_ribbon(aes(ymin=low,ymax=high), alpha = 0.3, fill= "orange", size = 0.8)+
    theme_bw()+
    theme(legend.position = "none")+
    labs(title = "Biden's Net Approval Ratings in 2022",
         subtitle = "Weekly Data, Approve - Disapprove, %",
         x = "Week in 2022", 
         y = NULL,
         caption = "Source: https://projects.fivethirtyeight.com/biden-approval-data/")

```

# Challenge 1: Excess rentals in TfL bike sharing

Recall the TfL data on how many bikes were hired every single day. We
can get the latest data by running the following

```{r, get_tfl_data, cache=TRUE}

url <- "https://data.london.gov.uk/download/number-bicycle-hires/ac29363e-e0cb-47cc-a97a-e216d900a6b0/tfl-daily-cycle-hires.xlsx"

# Download TFL data to temporary file
httr::GET(url, write_disk(bike.temp <- tempfile(fileext = ".xlsx")))

# Use read_excel to read it as dataframe
bike0 <- read_excel(bike.temp,
                   sheet = "Data",
                   range = cell_cols("A:B"))

# change dates to get year, month, and week
bike <- bike0 %>% 
  clean_names() %>% 
  rename (bikes_hired = number_of_bicycle_hires) %>% 
  mutate (year = year(day),
          month = lubridate::month(day, label = TRUE),
          week = isoweek(day))

# There are weeks labeled as 52 in Jan, we should fix it to 5
bike[(bike$year == 2022)&(bike$week != 52),]
```

We can easily create a facet grid that plots bikes hired by month and
year since 2015

```{r tfl_month_year_grid, echo=FALSE, out.width="100%"}
knitr::include_graphics(here::here("images", "tfl_distributions_monthly.png"), error = FALSE)
```

However, the challenge I want you to work on is to reproduce the
following two graphs.

```{r tfl_absolute_monthly_change, echo=FALSE, out.width="100%"}
knitr::include_graphics(here::here("images", "tfl_monthly.png"), error = FALSE)
```

```{r graph1, fig.height = 4, fig.width = 8}

# Find the expected monthly rental from 2016 to 2019
expected_mean <- bike %>% 
  filter(year %in% 2016:2019) %>% 
  group_by(month) %>% 
  summarize(expected_monthly_rental = mean(bikes_hired))

# join the expected value to the row data
joined_bike <- left_join(bike,expected_mean,"month")

joined_bike %>% 
  filter(year %in% 2017:2022) %>% 
  group_by(year,month) %>% 
  # Calculate the actual monthly mean value from 2017 to 2022
  mutate(actual_monthly_rental = mean(bikes_hired)) %>% 
  # Calculate the difference between actual and expected
  mutate(diff = actual_monthly_rental - 
           expected_monthly_rental) %>%
  ggplot(aes(month,group = 1)) + 
  # Fill the difference red if the expected value is larger
  geom_ribbon(aes(ymax = expected_monthly_rental,
                  ymin = pmin(diff,0)+expected_monthly_rental),
              fill = "red",alpha = 0.2)+
  # Fill the difference green if the actual value is larger
  geom_ribbon(aes(ymax = actual_monthly_rental,
                  ymin = actual_monthly_rental - pmax(diff,0)),
              fill = "green",alpha = 0.2)+
  geom_line(aes(y = expected_monthly_rental),color = "blue",size = 1.4) +
  geom_line(aes(y = actual_monthly_rental),size = 0.7)+
  facet_wrap(~year,nrow = 2)+
  labs(title="Monthly changes in TfL bike rentals",
       subtitle = "Change from monthly average shown in blue \nand calculated between 2016-2019",
       x=NULL,y = "Bike rentals",
       caption = "\n Source: TfL, London Data Store")+
  theme(plot.title.position = "plot")+
  theme(legend.position = "none")+
  theme_minimal()

```

The second one looks at percentage changes from the expected level of
weekly rentals. The two grey shaded rectangles correspond to Q2 (weeks
14-26) and Q4 (weeks 40-52).

```{r tfl_percent_change, echo=FALSE, out.width="100%"}
knitr::include_graphics(here::here("images", "tfl_weekly.png"), error = FALSE)
```

```{r graph2,warning=FALSE, fig.height = 4}

# Find the expected weekly rental from 2016 to 2019
expected_weekly <- bike %>% 
  filter(year %in% 2016:2019) %>% 
  group_by(week) %>% 
  summarize(expected_weekly_rental = mean(bikes_hired))

# join the expected weekly value to the row data
joined_weekly <- left_join(bike,expected_weekly,"week")

pct_change <- joined_weekly %>% 
  filter(year %in% 2017:2022) %>% 
  group_by(year,week) %>% 
  # The weekly change from the expected value
  summarize(weekly_change = 
              (mean(bikes_hired)-mean(expected_weekly_rental))/
              mean(expected_weekly_rental)) %>% 
  # Add a column to indicate whether the change is positive or negative
  mutate(p_or_n = case_when(weekly_change<0 ~ "Negative",
                            weekly_change > 0 ~ "Positive"))



pct_change %>% 
  ggplot(aes(x = week,y = weekly_change))+
  # Fill the back ground grey for Q2 and Q4
  geom_rect(aes(xmin = 13,xmax = 26,ymin = -0.6, ymax = 1),
            fill = "grey90",alpha = 0.04)+
  geom_rect(aes(xmin = 39,xmax = 52,ymin = -0.6, ymax = 1),
            fill = "grey90",alpha = 0.04)+
  geom_line()+
  # Fill the area below line red if the change is positive
  geom_ribbon(aes(ymax = pmax(weekly_change,0), ymin = 0),fill = "green",alpha = 0.2)+
  # Fill the area below line green if the change is negative
  geom_ribbon(aes(ymax = pmin(weekly_change,0), ymin = 0),fill = "red",alpha = 0.2)+
  facet_wrap(~year, nrow = 2)+
  # Add rugs at the bottom indicating the value according to p_or_n
  geom_rug(mapping = aes(color = factor(p_or_n)), sides = "b",show.legend = FALSE) +
  # Set the rug value to red and green
  scale_color_manual(values = c("red", "green")) +
  scale_x_continuous(breaks=seq(13, 53, 13))+
  scale_y_continuous(breaks=seq(-0.5, 1, 0.5),
                     limits = c(-0.6,1),
                     labels = scales::percent)+
  labs(title="Weekly changes in TfL bike rentals",
       subtitle = "% change from weekly averages \ncalculated between 2016-2019",
       x="week",y = NULL,
       caption = "Source: TfL, London Data Store")+
  theme(plot.title.position = "plot")+
  theme(legend.position = "none")+
  theme_minimal()

```


For both of these graphs, you have to calculate the expected number of
rentals per week or month between 2016-2019 and then, see how each
week/month of 2020-2022 compares to the expected rentals. Think of the
calculation `excess_rentals = actual_rentals - expected_rentals`.

Should you use the mean or the median to calculate your expected
rentals? Why?

# Challenge 2: Share of renewable energy production in the world

The National Bureau of Economic Research (NBER) has a a very interesting
dataset on the adoption of about 200 technologies in more than 150
countries since 1800. This is the[Cross-country Historical Adoption of
Technology (CHAT)
dataset](https://www.nber.org/research/data/cross-country-historical-adoption-technology).

The following is a description of the variables

| **variable** | **class** | **description**                |
|--------------|-----------|--------------------------------|
| variable     | character | Variable name                  |
| label        | character | Label for variable             |
| iso3c        | character | Country code                   |
| year         | double    | Year                           |
| group        | character | Group (consumption/production) |
| category     | character | Category                       |
| value        | double    | Value (related to label)       |

```{r,load_technology_data}

technology <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-07-19/technology.csv')

# Get all technologies
labels <- technology %>% 
  distinct(variable, label)

# Get country names using 'countrycode' package
technology <- technology %>% 
  filter(iso3c != "XCD") %>% 
  mutate(iso3c = recode(iso3c, "ROM" = "ROU"),
         country = countrycode(iso3c, origin = "iso3c", destination = "country.name"),
         country = case_when(
           iso3c == "ANT" ~ "Netherlands Antilles",
           iso3c == "CSK" ~ "Czechoslovakia",
           iso3c == "XKX" ~ "Kosovo",
           TRUE           ~ country))

# Make smaller dataframe on energy
energy <- technology %>% 
  filter(category == "Energy")

# Download CO2 per capita from World Bank using {wbstats} package
# https://data.worldbank.org/indicator/EN.ATM.CO2E.PC
co2_percap <- wb_data(country = "countries_only", 
                      indicator = "EN.ATM.CO2E.PC", 
                      start_date = 1970, 
                      end_date = 2022,
                      return_wide=FALSE) %>% 
  filter(!is.na(value)) %>% 
  #drop unwanted variables
  select(-c(unit, obs_status, footnote, last_updated))

# Get a list of countries and their characteristics
# We just want to get the region a country is in and its income level
countries <-  wb_cachelist$countries %>% 
  select(iso3c,region,income_level)

```

This is a very rich data set, not just for energy and CO2 data, but for
many other technologies. In our case, we just need to produce a couple
of graphs-- at this stage, the emphasis is on data manipulation, rather
than making the graphs gorgeous.

First, produce a graph with the countries with the highest and lowest %
contribution of renewables in energy production. This is made up of
`elec_hydro`, `elec_solar`, `elec_wind`, and `elec_renew_other`. You may
want to use the *patchwork* package to assemble the two charts next to
each other.

```{r min-max_renewables, echo=FALSE, out.width="100%", fig.width = 10.3, fig.height = 5.5}
knitr::include_graphics(here::here("images", "renewables.png"), error = FALSE)

# Select relevant columns and pivot data frame into wide format
energy_wider_2019 <- energy %>% 
  filter(year == 2019) %>% 
  select(country, value, variable) %>% 
  pivot_wider(names_from = "variable",
              values_from = "value") %>% 
  replace(is.na(.),0) %>% 
# Calculate percentage of renewable energy
  mutate(total_perc_renew = ((elec_hydro + elec_solar + elec_wind + elec_renew_other)/elecprod),
         country = fct_reorder(country,total_perc_renew)) %>%
# Filter out countries with no renewable energy production in 2019 & remove North Macedonia because of wronly calculated elecprod
  filter(total_perc_renew > 0, total_perc_renew != Inf)

# Create max graph
top20_renewable <- energy_wider_2019 %>% 
  slice_max(total_perc_renew, n = 20) %>% 
  ggplot(aes(x = total_perc_renew, y = country)) +
    geom_col() +
    theme_bw() +
    theme(panel.border = element_blank()) +
    labs(x = NULL, y = NULL) + 
    scale_x_continuous(labels = scales::percent_format())

# Create min graph
bottom20_renewable <- energy_wider_2019 %>% 
  slice_min(total_perc_renew, n = 20) %>% 
  ggplot(aes(x = total_perc_renew, y = country)) +
    geom_col() +
    theme_bw() +
    theme(panel.border = element_blank()) +
    labs(x = NULL, y = NULL) +
    scale_x_continuous(labels = scales::percent_format())

# Combine graphs into one with patchwork
combined_renew_graph <- top20_renewable + bottom20_renewable

# Add labels to the combined graph
combined_renew_graph +
  plot_annotation(title = "Highest and lowest % of renewables in energy production",
       subtitle = "2019 data",
       caption = "Source: NBER CHAT Database")

```

Second, you can produce an animation to explore the relationship between
CO2 per capita emissions and the deployment of renewables. As the % of
energy generated by renewables goes up, do CO2 per capita emissions seem
to go down?

If percentage of renewable energy produced would have a negative correlation with CO2 emissions per capita, we would expect the data points to move along a negative slope from the top left towards the bottom right of the chart. Our observations yield interesting insights. When looking at poorer countries (low and lower middle income), we see generally lower CO2 emissions, but also cannot claim any significant effect of percentage of renewable energy produced on these emissions. The data points move along a horizontal line. In contrast, the wealthier countries (upper middle and high income) exhibit higher emissions, but we can see a slight correlation between the graphed variables. Nevertheless, this assertion should be viewed with a sceptical eye before assessing whether the correlation is statistically significant. The graphs provide a good first indication, but further analysis is recommended.

```{r animation, echo=FALSE, out.width="100%", fig.width = 7}
knitr::include_graphics(here::here("images", "animation.gif"), error = FALSE)

# Find renewable energy % since 1990
energy_since_1990 <- energy %>% 
  filter(year >= 1990) %>%
  select(iso3c, year, value, variable) %>%
  # Pivot wider to later calculate renewable energy %
  pivot_wider(names_from = "variable",
              values_from = "value") %>% 
  # Replace missing values with 0
  replace(is.na(.),0) %>% 
  # Calculate percentage of renewable energy
  mutate(total_perc_renew = ((elec_hydro + elec_solar + elec_wind + elec_renew_other)/elecprod)) %>% 
  # Select only necessary columns
  select(iso3c, year, total_perc_renew) %>% 
  # Remove infinite values because of faulty 0 value in total energy production
  filter(total_perc_renew <= 1)

# Join CO2 emissions data with country and renewable energy % data
co2_percap_joined <- co2_percap %>% 
  rename("year" = "date") %>% 
  left_join(countries, by = "iso3c") %>% 
  left_join(energy_since_1990, by = c("iso3c","year"))

# Create faceted scatterplot for CO2 emissions and renewable energy %
co2_percap_joined %>% 
  mutate(year = as.integer(year)) %>% 
  ggplot(aes(x = total_perc_renew, y = value, color = income_level)) +
  geom_point() +
  facet_wrap(~income_level) +
  # Add animation
  labs(title = 'Year: {frame_time}', 
       x = '% renewables', 
       y = 'CO2 per cap') +
       transition_time(year) +
       ease_aes('linear') +
  scale_x_continuous(labels = scales::percent_format()) +
  theme_bw() +
  theme(legend.position = "none")

```

To create this animation is actually straight-forward. You manipulate
your date, and then create the graph in the normal ggplot way. the only
`gganimate` layers you need to add to your graphs are

      labs(title = 'Year: {frame_time}', 
           x = '% renewables', 
           y = 'CO2 per cap') +
      transition_time(year) +
      ease_aes('linear')

# Deliverables

As usual, there is a lot of explanatory text, comments, etc. You do not
need these, so delete them and produce a stand-alone document that you
could share with someone. Knit the edited and completed R Markdown file
as an HTML document (use the "Knit" button at the top of the script
editor window) and upload it to Canvas.

# Details

-   Who did you collaborate with: Group 8 team members as stated at the start of document
-   Approximately how much time did you spend on this problem set: 3 hours each
-   What, if anything, gave you the most trouble: Challenge 2

**Please seek out help when you need it,** and remember the [15-minute
rule](https://mam202.netlify.app/syllabus/#the-15-minute-rule){target="_blank"}.
You know enough R (and have enough examples of code from class and your
readings) to be able to do this. If you get stuck, ask for help from
others, post a question on Slack-- and remember that I am here to help
too!

> As a true test to yourself, do you understand the code you submitted
> and are you able to explain it to someone else?

# Rubric

Check minus (1/5): Displays minimal effort. Doesn't complete all
components. Code is poorly written and not documented. Uses the same
type of plot for each graph, or doesn't use plots appropriate for the
variables being analyzed.

Check (3/5): Solid effort. Hits all the elements. No clear mistakes.
Easy to follow (both the code and the output).

Check plus (5/5): Finished all components of the assignment correctly
and addressed both challenges. Code is well-documented (both
self-documented and with additional comments as necessary). Used
tidyverse, instead of base R. Graphs and tables are properly labelled.
Analysis is clear and easy to follow, either because graphs are labeled
clearly or you've written additional text to describe how you interpret
the output.
